{
  "name": "vEMDiffuse-a",
  // experiments name
  "norm": true,
  "percent": false,
  "gpu_ids": [
    0,
    1
  ],
  // gpu ids list, default is single 0
  "seed": -1,
  // random seed, seed <0 represents randomization not used
  "finetune_norm": false,
  // find the parameters to optimize
  "task": "3d_reconstruction",
  "path": {
    //set every part file path
    "base_dir": "experiments",
    // base path for all log except resume_state
    "code": "code",
    // code backup
    "tb_logger": "tb_logger",
    // path of tensorboard logger
    "results": "results",
    "checkpoint": "checkpoint",
//    "resume_state": "experiments/emdiffusie-a-phlep/4860"
    "resume_state": "experiments/vEMDiffuse-a/best"

//      "resume_state": null // ex: 100, loading .state  and .pth from given epoch and iteration
  },
  "datasets": {
    // train or test
    "train": {
      "which_dataset": {
        // import designated dataset using arguments
        "name": [
          "data.dataset",
          "vEMDiffuseTrainingDatasetVolume"
        ],
        // import Dataset() class / function(not recommend) from data.dataset.py (default is [data.dataset.py])
        "args": {
          // arguments to initialize dataset
          "data_root": "/data/cxlu/phelps_test_patches_6144/",
          "data_len": -1,
          "norm": true,
          "percent": false,
          "z_times": 10,
          "method": "vEMDiffuse-a",
          "image_size": [256, 256]
        }
      },
      "dataloader": {
        "validation_split": 20,
        // percent or number
        "args": {
          // arguments to initialize train_dataloader
          "batch_size": 3,
          // batch size in each gpu
          "num_workers": 2,
          "shuffle": true,
          "pin_memory": false,
          "drop_last": true
        },
        "val_args": {
          // arguments to initialize valid_dataloader, will overwrite the parameters in train_dataloader
          "batch_size": 10,
          // batch size in each gpu
          "num_workers": 2,
          "shuffle": false,
          "pin_memory": false,
          "drop_last": false
        }
      }
    },
    "test": {
      "which_dataset": {
        "name": "vEMDiffuseTestAnIsotropic",
        // import Dataset() class / function(not recommend) from default file
        "args": {
//          "data_root": "/data/cxlu/phelps_test_patches_2048/",
          "data_root": "/mnt/sdb/cxlu/phelps_test_patches_6144/",
          "norm": true,
          "percent": false,
          "phase": "val",
          "z_times": 10
        }
      },
      "dataloader": {
        "args": {
          "batch_size": 8,
          "num_workers": 0,
          "pin_memory": true
        }
      }
    }
  },
  "model": {
    // networks/metrics/losses/optimizers/lr_schedulers is a list and model is a dict
    "which_model": {
      // import designated  model(trainer) using arguments
      "name": [
        "models.vEMDiffuse_model",
        "DiReP"
      ],
      // import Model() class / function(not recommend) from models.EMDiffuse_model.py (default is [models.EMDiffuse_model.py])
      "args": {
        "sample_num": 8,
        // process of each image
        "task": "3d_reconstruct",
        "ema_scheduler": {
          "ema_start": 1,
          "ema_iter": 1,
          "ema_decay": 0.9999
        },
        "optimizers": [
          {
            "lr": 5e-5,
            "weight_decay": 0
          }
        ]
      }
    },
    "which_networks": [
      // import designated list of networks using arguments
      {
        "name": [
          "models.vEMDiffuse_network",
          "Network"
        ],
        // import Network() class / function(not recommend) from default file (default is [models/EMDiffuse_network.py])
        "args": {
          // arguments to initialize network
          "init_type": "kaiming",
          // method can be [normal | xavier| xavier_uniform | kaiming | orthogonal], default is kaiming
          "module_name": "guided_diffusion_3d_2d",
          // sr3 | guided_diffusion
          "norm": true,
          "unet": {
            "in_channel": 3,
            "out_channel": 1,
            "inner_channel": 32,
            "channel_mults": [
              1,
              2,
              4,
              8
            ],
            "attn_res": [
              // 32,
              16
              // 8
            ],
            "num_head_channels": 32,
            "res_blocks": 2,
            "dropout": 0.2,
            "image_size": 256
          },
          "beta_schedule": {
            "train": {
              "schedule": "linear",
              "n_timestep": 2000,
              // "n_timestep": 5, // debug
              "linear_start": 1e-6,
              "linear_end": 0.01
            },
            "test": {
              "schedule": "linear",
              "n_timestep": 1000,
              // "n_timestep": 5, // debug
              "linear_start": 1e-4,
              "linear_end": 0.09
            }
          }
        }
      }
    ],
    "which_losses": [
      // import designated list of losses without arguments
      "mse_loss"
      // import mse_loss() function/class from default file (default is [models/losses.py]), equivalent to { "name": "mse_loss", "args":{}}
    ],
    "which_metrics": [
      // import designated list of metrics without arguments
      "mae"
      // import mae() function/class from default file (default is [models/metrics.py]), equivalent to { "name": "mae", "args":{}}
    ]
  },
  "train": {
    // arguments for basic training
    "n_epoch": 1e8,
    // max epochs, not limited now
    "n_iter": 1e8,
    // max interations
    "val_epoch": 20,
    // valdation every specified number of epochs
    "save_checkpoint_epoch": 20,
    "log_iter": 1e4,
    // log every specified number of iterations
    "tensorboard": true
    // tensorboardX enable
  },
  "debug": {
    // arguments in debug mode, which will replace arguments in train
    "val_epoch": 1,
    "save_checkpoint_epoch": 1,
    "log_iter": 10,
    "debug_split": 50
    // percent or number, change the size of dataloder to debug_split.
  }
}
